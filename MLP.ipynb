{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cbab5e7-d20b-4e3b-b5d3-c628abce3685",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/derecho/scratch/pahlavan/tmp/ipykernel_104821/2326193737.py:9: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was too old on your system - pyarrow 10.0.1 is the current minimum supported version as of this release.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# Importing the requried packages\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.utils.prune as prune\n",
    "from matplotlib.ticker import (MultipleLocator, AutoMinorLocator)\n",
    "from mpl_toolkits.axes_grid1.inset_locator import (inset_axes, InsetPosition, mark_inset)\n",
    "import matplotlib\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "from qbo1d import adsolver, utils\n",
    "import netCDF4 as nc\n",
    "\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import matplotlib\n",
    "import netCDF4 as nc\n",
    "import scipy.stats as st\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2a9c734-06bf-4ce4-8c88-c7687c7cd1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for scaling input or output\n",
    "class GlobalScaler():\n",
    "    def __init__(self, X):\n",
    "        self.abs_max = X.std().max() #(0)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return X / self.abs_max\n",
    "    \n",
    "    def inverse_transform(self, X):\n",
    "        return X * self.abs_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0872528-4242-4cbf-a0b5-80ffa8282334",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = adsolver.ADSolver(z_min=17e3, z_max=35e3, dz=500, t_min=0.0, t_max=365*100*86400, dt=86400.0, w =0.0, kappa=3e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1709925-7d6c-4b18-8ef2-dca7dcd0147b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the training data\n",
    "data = xr.open_dataset('../QBO_0.5x_100yrs_500m.nc')\n",
    "u = data.u[:-1, 1:-1].values\n",
    "f = data.f[1:, 1:-1].values\n",
    "\n",
    "trainset = TensorDataset(torch.from_numpy(u[:90*365]).float(),torch.from_numpy(f[:90*365]).float())\n",
    "valset = TensorDataset(torch.from_numpy(u[90*365:]).float(),torch.from_numpy(f[90*365:]).float())\n",
    "\n",
    "train_dataloader = DataLoader(trainset, batch_size=1000, shuffle=True, num_workers=4)\n",
    "val_dataloader  = DataLoader(valset, batch_size= len(valset), shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4eab320-26f4-4e87-b7a9-e2418f7c915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the testing data\n",
    "data = xr.open_dataset('/glade/derecho/scratch/pahlavan/qbo1d/Nonlocality-0.5x/QBO_0.5x_1000yrs_500m.nc')\n",
    "u = data.u[:-1, 1:-1].values\n",
    "f = data.f[1:, 1:-1].values\n",
    "\n",
    "testset = TensorDataset(torch.from_numpy(u).float(),torch.from_numpy(f).float())\n",
    "test_dataloader  = DataLoader(testset, batch_size= len(testset), shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6b45c28-e447-4b52-9738-530883fe6103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP arcitecture\n",
    "\n",
    "nch = 156\n",
    "actv = nn.Tanh()\n",
    "\n",
    "\n",
    "class FullyConnected(nn.Module):\n",
    "    def __init__(self, solver, scaler_X=None, scaler_Y=None):\n",
    "        super(FullyConnected, self).__init__()\n",
    "\n",
    "        self.scaler_X = scaler_X\n",
    "        self.scaler_Y = scaler_Y\n",
    "\n",
    "        nlev = solver.z.shape[0]\n",
    "        \n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=nlev-2, out_features=nch),\n",
    "            actv,\n",
    "            nn.Linear(in_features=nch, out_features=nch),\n",
    "            actv,\n",
    "            nn.Linear(in_features=nch, out_features=nch),\n",
    "            actv,\n",
    "            nn.Linear(in_features=nch, out_features=nlev-2)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear_stack(X[:, None]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff69f56d-89ad-4c19-be56-4793272bf1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FullyConnected(\n",
       "  (linear_stack): Sequential(\n",
       "    (0): Linear(in_features=35, out_features=156, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=156, out_features=156, bias=True)\n",
       "    (3): Tanh()\n",
       "    (4): Linear(in_features=156, out_features=156, bias=True)\n",
       "    (5): Tanh()\n",
       "    (6): Linear(in_features=156, out_features=35, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = (FullyConnected(solver)).float()\n",
    "device = \"cpu\"\n",
    "print(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ce2a1ab-5486-4fe3-9804-c82472d8441c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60095"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nparams\n",
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a68557f-e229-4f79-9c62-5c45da91e924",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "            \n",
    "            # save model\n",
    "            torch.save(model.state_dict(), '../MLP_4L_156n_0.5x_500m.pth')\n",
    "\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b841a72-bf2b-45e0-a646-c5d5c42470c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    avg_loss = 0\n",
    "    for batch, (X, Y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X.to(device).float())\n",
    "        loss = loss_fn(pred.float(), scaler_Y.transform(Y.to(device)).float())\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "   \n",
    "        with torch.no_grad():\n",
    "            avg_loss += loss.item()\n",
    "            \n",
    "    avg_loss /= len(dataloader)\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "# validating loop\n",
    "def val_loop(dataloader, model, loss_fn):\n",
    "    avg_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, Y) in enumerate(dataloader):\n",
    "            # Compute prediction and loss\n",
    "            pred = model(X.to(device).float())\n",
    "            loss = loss_fn(pred.float(), scaler_Y.transform(Y.to(device)).float())\n",
    "            avg_loss += loss.item()\n",
    "            \n",
    "    avg_loss /= len(dataloader)\n",
    "    \n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5d0a43a-4765-42d2-a996-75d8aeaa220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "37b3c506-d80c-4c7e-9034-601017497d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "epochs = 6000\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scaler_Y = GlobalScaler(trainset[:][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "61da5b3b-514d-4815-8cb5-bc302dca67d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "0.031848274171352386\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "0.01798270083963871\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "0.01940186135470867\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "0.018415361642837524\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "0.017620276659727097\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "0.01802794635295868\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "0.018582139164209366\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "0.018108122050762177\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "0.019032729789614677\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "0.019124334678053856\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "0.01948104240000248\n",
      "New Learning Rate: 0.0025\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "0.014225199818611145\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "0.014129542745649815\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "0.014381526038050652\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "0.014604523777961731\n",
      "New Learning Rate: 0.000625\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "0.013843805529177189\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "0.014038614928722382\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "0.014021082781255245\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "0.014167863875627518\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "0.01414338406175375\n",
      "New Learning Rate: 0.00015625\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "0.01394484844058752\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "0.013842595741152763\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "0.013912107795476913\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "0.013962529599666595\n",
      "New Learning Rate: 3.90625e-05\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "0.013917629607021809\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "0.013893421739339828\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "0.013891773298382759\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "0.01388498954474926\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "0.013889315538108349\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "0.013891605660319328\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "0.013884511776268482\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "0.013877532444894314\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "0.013894875533878803\n",
      "New Learning Rate: 9.765625e-06\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "0.01390107162296772\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "0.013902062550187111\n",
      "Epoch 351\n",
      "-------------------------------\n",
      "0.013894777745008469\n",
      "Epoch 361\n",
      "-------------------------------\n",
      "0.013888953253626823\n",
      "Epoch 371\n",
      "-------------------------------\n",
      "0.013889197260141373\n",
      "New Learning Rate: 2.44140625e-06\n",
      "Epoch 381\n",
      "-------------------------------\n",
      "0.013889831490814686\n",
      "Epoch 391\n",
      "-------------------------------\n",
      "0.01389459427446127\n",
      "Epoch 401\n",
      "-------------------------------\n",
      "0.013895401731133461\n",
      "Epoch 411\n",
      "-------------------------------\n",
      "0.013893992640078068\n",
      "Epoch 421\n",
      "-------------------------------\n",
      "0.013899270445108414\n",
      "New Learning Rate: 6.103515625e-07\n",
      "Epoch 431\n",
      "-------------------------------\n",
      "0.013896317221224308\n",
      "Epoch 441\n",
      "-------------------------------\n",
      "0.013896241784095764\n",
      "Epoch 451\n",
      "-------------------------------\n",
      "0.013895722106099129\n",
      "Epoch 461\n",
      "-------------------------------\n",
      "0.01389613002538681\n",
      "New Learning Rate: 1.52587890625e-07\n",
      "Epoch 471\n",
      "-------------------------------\n",
      "0.013895288109779358\n",
      "Epoch 481\n",
      "-------------------------------\n",
      "0.01389553677290678\n",
      "Epoch 491\n",
      "-------------------------------\n",
      "0.013895468786358833\n",
      "Epoch 501\n",
      "-------------------------------\n",
      "0.01389556284993887\n",
      "Epoch 511\n",
      "-------------------------------\n",
      "0.013895371928811073\n",
      "Epoch 521\n",
      "-------------------------------\n",
      "0.013895559124648571\n",
      "Epoch 531\n",
      "-------------------------------\n",
      "0.01389578077942133\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "k = 0\n",
    "early_stopper = EarlyStopper(patience=40, min_delta=0.0)\n",
    "for t in range(epochs):\n",
    "    train_loss = train_loop(train_dataloader, model, nn.MSELoss(), optimizer)\n",
    "    train_losses.append(train_loss)\n",
    "    val_loss = val_loop(val_dataloader, model, nn.MSELoss())\n",
    "    val_losses.append(val_loss)\n",
    "    if t % 10 ==0:\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        print(val_loss)\n",
    "        # print(train_loss)\n",
    "    if early_stopper.early_stop(val_loss):\n",
    "        if k <8:\n",
    "            early_stopper = EarlyStopper(patience=40, min_delta=0.0)\n",
    "            learning_rate = learning_rate * 0.25\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            k += 1\n",
    "            print(\"New Learning Rate: \" + str(learning_rate))\n",
    "        else:\n",
    "            break\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "torch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
