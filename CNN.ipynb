{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cbab5e7-d20b-4e3b-b5d3-c628abce3685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the requried packages\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.utils.prune as prune\n",
    "from matplotlib.ticker import (MultipleLocator, AutoMinorLocator)\n",
    "from mpl_toolkits.axes_grid1.inset_locator import (inset_axes, InsetPosition, mark_inset)\n",
    "import matplotlib\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "from qbo1d import adsolver, utils\n",
    "import netCDF4 as nc\n",
    "\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import matplotlib\n",
    "import netCDF4 as nc\n",
    "import scipy.stats as st\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2a9c734-06bf-4ce4-8c88-c7687c7cd1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for scaling input or output\n",
    "\n",
    "class GlobalScaler():\n",
    "    def __init__(self, X):\n",
    "        self.abs_max = X.std().max() #(0)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return X / self.abs_max\n",
    "    \n",
    "    def inverse_transform(self, X):\n",
    "        return X * self.abs_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e6f2de8-6d98-46a4-8fbf-740d480744dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the resolution of the 1D model\n",
    "rez=125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbcf63f-ab02-4460-89c7-25685b4ea503",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = adsolver.ADSolver(z_min=17e3, z_max=35e3, dz=rez, t_min=0.0, t_max=365*100*86400, dt=86400.0, w =0.0, kappa=3e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1709925-7d6c-4b18-8ef2-dca7dcd0147b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the training data\n",
    "\n",
    "data = xr.open_dataset('/glade/derecho/scratch/pahlavan/qbo1d/Nonlocality-0.5x/QBO_0.5x_100yrs_' + str(rez) + 'm.nc')\n",
    "u = data.u[:-1, 1:-1].values\n",
    "f = data.f[1:, 1:-1].values\n",
    "\n",
    "trainset = TensorDataset(torch.from_numpy(u[:90*365]).float(),torch.from_numpy(f[:90*365]).float())\n",
    "valset = TensorDataset(torch.from_numpy(u[90*365:]).float(),torch.from_numpy(f[90*365:]).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec0ddc4c-887f-407a-8592-8b963877a851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaler_Y:tensor(3.2622e-06)\n"
     ]
    }
   ],
   "source": [
    "scaler_Y = GlobalScaler(trainset[:][1])\n",
    "print('scaler_Y:' + str(scaler_Y.abs_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8130cc31-73fe-4dd0-af4d-f7b66963b6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(trainset, batch_size=1000, shuffle=True, num_workers=4)\n",
    "val_dataloader  = DataLoader(valset, batch_size= len(valset), shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4eab320-26f4-4e87-b7a9-e2418f7c915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the testing data\n",
    "\n",
    "data = xr.open_dataset('/glade/derecho/scratch/pahlavan/qbo1d/Nonlocality-0.5x/QBO_0.5x_1000yrs_'+ str(rez) + 'm.nc')\n",
    "u = data.u[:-1, 1:-1].values\n",
    "f = data.f[1:, 1:-1].values\n",
    "\n",
    "# u\n",
    "testset = TensorDataset(torch.from_numpy(u).float(),torch.from_numpy(f).float())\n",
    "test_dataloader  = DataLoader(testset, batch_size= len(testset), shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bba44232-315c-424b-aac4-3826ce16053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "actv = nn.Tanh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0041384-ed7c-4bb5-ac06-9527d18c410d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different configurations are used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40f7ec9d-123c-4daa-9eb9-038ab5056a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {'1': {'nl':9, 'nch':11, 'ks':17, 'dil':1},\n",
    "           '2': {'nl':9, 'nch':11, 'ks':19, 'dil':1},\n",
    "           '3': {'nl':9, 'nch':10, 'ks':21, 'dil':1},\n",
    "           '4': {'nl':9, 'nch':9, 'ks':23, 'dil':1},\n",
    "           '5': {'nl':9, 'nch':9, 'ks':25, 'dil':1},\n",
    "           \n",
    "           '6': {'nl':10, 'nch':11, 'ks':17, 'dil':1},\n",
    "           '7': {'nl':11, 'nch':10, 'ks':17, 'dil':1},\n",
    "           '8': {'nl':12, 'nch':9, 'ks':17, 'dil':1},\n",
    "           '9': {'nl':13, 'nch':9, 'ks':17, 'dil':1},\n",
    "           \n",
    "           '10': {'nl':8, 'nch':16, 'ks':9, 'dil':2},\n",
    "           '11': {'nl':8, 'nch':16, 'ks':9, 'dil':3},\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b05ba50-45c2-482c-b235-a03a8d4d59c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {'1': {'nl':14, 'nch':10, 'ks':13, 'dil':1},\n",
    "           '2': {'nl':15, 'nch':9, 'ks':13, 'dil':1}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "013e6793-efca-4dc4-95f2-7e54333ab4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {'100': {'1': {'nl':8, 'nch':16, 'ks':9, 'dil':2},\n",
    "                   '2': {'nl':9, 'nch':13, 'ks':13, 'dil':2}},\n",
    "           '125':{'1': {'nl':9, 'nch':14, 'ks':11, 'dil':2}},\n",
    "           '150':{'1': {'nl':8, 'nch':15, 'ks':11, 'dil':2}},\n",
    "           '200':{'1': {'nl':6, 'nch':18, 'ks':11, 'dil':2}},\n",
    "           '300':{'1': {'nl':5, 'nch':23, 'ks':9, 'dil':2}},\n",
    "           '400':{'1': {'nl':4, 'nch':28, 'ks':9, 'dil':2}},\n",
    "           '600':{'1': {'nl':4, 'nch':32, 'ks':7, 'dil':2}},\n",
    "           '750':{'1': {'nl':4, 'nch':38, 'ks':5, 'dil':2}},\n",
    "           '900':{'1': {'nl':4, 'nch':38, 'ks':5, 'dil':2}},\n",
    "           '1200':{'1': {'nl':3, 'nch':54, 'ks':5, 'dil':2}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59aa22cc-c30e-4993-a318-bb4a0633fa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {'600': {'1': {'nl':4, 'nch':28, 'ks':9, 'dil':1},\n",
    "                   '2': {'nl':8, 'nch':33, 'ks':5, 'dil':1}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "079ec689-8692-49dd-9e8d-cdd01cb50932",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {'100':{'1': {'nl':4, 'nch':19, 'ks':19, 'dil':1}},\n",
    "           '125':{'1': {'nl':4, 'nch':19, 'ks':19, 'dil':1}},\n",
    "           '150':{'1': {'nl':4, 'nch':19, 'ks':19, 'dil':1}},\n",
    "           '200':{'1': {'nl':4, 'nch':19, 'ks':19, 'dil':1}},\n",
    "           '250':{'1': {'nl':4, 'nch':19, 'ks':19, 'dil':1}},\n",
    "           '300':{'1': {'nl':4, 'nch':19, 'ks':19, 'dil':1}},\n",
    "           '400':{'1': {'nl':4, 'nch':19, 'ks':19, 'dil':1}},\n",
    "           '600':{'1': {'nl':4, 'nch':19, 'ks':19, 'dil':1}},\n",
    "           '750':{'1': {'nl':4, 'nch':19, 'ks':19, 'dil':1}},\n",
    "           '900':{'1': {'nl':4, 'nch':19, 'ks':19, 'dil':1}},\n",
    "           '1200':{'1': {'nl':4, 'nch':19, 'ks':19, 'dil':1}},\n",
    "           '1500':{'1': {'nl':4, 'nch':19, 'ks':19, 'dil':1}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eabfd409-80ad-4b61-af1d-139cf6e4a53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {'100': {'1': {'nl':9, 'nch':8, 'ks':29, 'dil':1}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2789a0e1-ac06-4df2-a301-a765948ec37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {'360': {'1': {'nl':5, 'nch':31, 'ks':5, 'dil':1},\n",
    "                   '2': {'nl':5, 'nch':23, 'ks':9, 'dil':1},\n",
    "                   '3': {'nl':5, 'nch':19, 'ks':13, 'dil':1},\n",
    "                   '4': {'nl':5, 'nch':17, 'ks':17, 'dil':1},\n",
    "                   '5': {'nl':5, 'nch':15, 'ks':21, 'dil':1},\n",
    "                   \n",
    "                   '6': {'nl':6, 'nch':20, 'ks':9, 'dil':1},\n",
    "                   '7': {'nl':7, 'nch':18, 'ks':9, 'dil':1},\n",
    "                   '8': {'nl':8, 'nch':16, 'ks':9, 'dil':1},\n",
    "                   '9': {'nl':9, 'nch':15, 'ks':9, 'dil':1},\n",
    "                   '10': {'nl':10, 'nch':14, 'ks':9, 'dil':1},\n",
    "                   \n",
    "                   '11': {'nl':5, 'nch':26, 'ks':7, 'dil':2},\n",
    "                   '12': {'nl':5, 'nch':26, 'ks':7, 'dil':3},\n",
    "                   '13': {'nl':4, 'nch':32, 'ks':7, 'dil':2}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5124c1b-ab3b-4024-b189-cecccbcd6d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {'125': {'1': {'nl':5, 'nch':18, 'ks':15, 'dil':2},\n",
    "                   '2': {'nl':6, 'nch':16, 'ks':15, 'dil':2},\n",
    "                   '3': {'nl':7, 'nch':14, 'ks':15, 'dil':2},\n",
    "                   '4': {'nl':8, 'nch':13, 'ks':15, 'dil':2},\n",
    "                   '5': {'nl':9, 'nch':12, 'ks':15, 'dil':2}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6b45c28-e447-4b52-9738-530883fe6103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The CNN for interpretability\n",
    "class FullyConnected(nn.Module):\n",
    "    def __init__(self, nl, nch, ks, dil):\n",
    "        super(FullyConnected, self).__init__()\n",
    "\n",
    "        self.linear_stack = nn.ModuleList()\n",
    "\n",
    "        self.linear_stack.append(nn.Conv1d(in_channels=1, out_channels=nch, kernel_size=ks, dilation=dil, padding=\"same\"))\n",
    "        self.linear_stack.append(actv)\n",
    "\n",
    "        for i in range(nl-2):\n",
    "            self.linear_stack.append(nn.Conv1d(in_channels=nch, out_channels=nch, kernel_size=ks, dilation=dil, padding=\"same\"))\n",
    "            self.linear_stack.append(actv)\n",
    "            \n",
    "        self.linear_stack.append(nn.Conv1d(in_channels=nch, out_channels=1, kernel_size=ks, dilation=dil, padding=\"same\"))\n",
    "\n",
    "    def forward(self, X):\n",
    "        x = X[:, None]\n",
    "        for layer in self.linear_stack:\n",
    "            x = layer(x)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a68557f-e229-4f79-9c62-5c45da91e924",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, nl, nch, ks, dil, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "            \n",
    "            # save model\n",
    "            torch.save(model.state_dict(), '/glade/derecho/scratch/pahlavan/qbo1d/Nonlocality-0.5x/NNs/Model/CNN_' + \n",
    "                       str(nl) + 'L_' + str(nch) + 'C_k' + str(ks) + '_D' + str(dil) + '_0.5x_' + str(rez) + 'm.pth')\n",
    "            \n",
    "            \n",
    "\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b841a72-bf2b-45e0-a646-c5d5c42470c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    avg_loss = 0\n",
    "    for batch, (X, Y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X.to(device).float())\n",
    "        loss = loss_fn(pred.float(), scaler_Y.transform(Y.to(device)).float())\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "   \n",
    "        with torch.no_grad():\n",
    "            avg_loss += loss.item()\n",
    "            \n",
    "    avg_loss /= len(dataloader)\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "# validating loop\n",
    "def val_loop(dataloader, model, loss_fn):\n",
    "    avg_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, Y) in enumerate(dataloader):\n",
    "            # Compute prediction and loss\n",
    "            pred = model(X.to(device).float())\n",
    "            loss = loss_fn(pred.float(), scaler_Y.transform(Y.to(device)).float())\n",
    "            avg_loss += loss.item()\n",
    "            \n",
    "    avg_loss /= len(dataloader)\n",
    "    \n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "61da5b3b-514d-4815-8cb5-bc302dca67d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "case5\n",
      "15577\n",
      "scaler_Y:tensor(3.2622e-06)\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "0.25814035534858704\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "0.024000143632292747\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "0.007515815086662769\n",
      "New Learning Rate: 0.0025\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "0.0060591562651097775\n",
      "Epoch 401\n",
      "-------------------------------\n",
      "0.0030676990281790495\n",
      "New Learning Rate: 0.000625\n",
      "Epoch 501\n",
      "-------------------------------\n",
      "0.0018914799438789487\n",
      "Epoch 601\n",
      "-------------------------------\n",
      "0.001767899258993566\n",
      "Epoch 701\n",
      "-------------------------------\n",
      "0.0016886385856196284\n",
      "New Learning Rate: 0.00015625\n",
      "Epoch 801\n",
      "-------------------------------\n",
      "0.001522848499007523\n",
      "Epoch 901\n",
      "-------------------------------\n",
      "0.0014905155403539538\n",
      "New Learning Rate: 3.90625e-05\n",
      "Epoch 1001\n",
      "-------------------------------\n",
      "0.0014578084228560328\n",
      "Epoch 1101\n",
      "-------------------------------\n",
      "0.001447984715923667\n",
      "New Learning Rate: 9.765625e-06\n",
      "New Learning Rate: 2.44140625e-06\n",
      "New Learning Rate: 6.103515625e-07\n",
      "Epoch 1201\n",
      "-------------------------------\n",
      "0.0014471790054813027\n",
      "New Learning Rate: 1.52587890625e-07\n",
      "Epoch 1301\n",
      "-------------------------------\n",
      "0.0014452411560341716\n",
      "Done!\n",
      "tensor(1.2394e-07)\n",
      "0.9985369601828615\n",
      "###############\n"
     ]
    }
   ],
   "source": [
    "for rez in [125]: #100, 125, 150, 200, 250, 300, 400, 600, 750, 900, \n",
    "    # for case in range(len(configs[str(rez)])):\n",
    "    for case in range(4, 5):\n",
    "\n",
    "        nl = configs[str(rez)][str(case+1)]['nl']\n",
    "        nch = configs[str(rez)][str(case+1)]['nch']\n",
    "        ks = configs[str(rez)][str(case+1)]['ks']\n",
    "        dil = configs[str(rez)][str(case+1)]['dil']\n",
    "    \n",
    "        \n",
    "        model = (FullyConnected(nl, nch, ks, dil)).float()\n",
    "        device = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda\"\n",
    "            if torch.cuda.device_count() > 1:\n",
    "                model = nn.DataParallel(model)\n",
    "        print(device)\n",
    "        model.to(device)\n",
    "    \n",
    "        #nparams\n",
    "        print('case' + str(case+1))\n",
    "        print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "    \n",
    "        data = xr.open_dataset('/glade/derecho/scratch/pahlavan/qbo1d/Nonlocality-0.5x/QBO_0.5x_100yrs_' + str(rez) + 'm.nc')\n",
    "        u = data.u[:-1, 1:-1].values\n",
    "        f = data.f[1:, 1:-1].values\n",
    "        \n",
    "        #big data\n",
    "        trainset = TensorDataset(torch.from_numpy(u[:90*365]).float(),torch.from_numpy(f[:90*365]).float())\n",
    "        valset = TensorDataset(torch.from_numpy(u[90*365:]).float(),torch.from_numpy(f[90*365:]).float())\n",
    "        \n",
    "        scaler_Y = GlobalScaler(trainset[:][1])\n",
    "        \n",
    "        train_dataloader = DataLoader(trainset, batch_size=1000, shuffle=True, num_workers=4)\n",
    "        val_dataloader  = DataLoader(valset, batch_size= len(valset), shuffle=True, num_workers=4)\n",
    "        \n",
    "        data = xr.open_dataset('/glade/derecho/scratch/pahlavan/qbo1d/Nonlocality-0.5x/QBO_0.5x_1000yrs_'+ str(rez) + 'm.nc')\n",
    "        u = data.u[:-1, 1:-1].values\n",
    "        f = data.f[1:, 1:-1].values\n",
    "        \n",
    "        # u\n",
    "        testset = TensorDataset(torch.from_numpy(u).float(),torch.from_numpy(f).float())\n",
    "        test_dataloader  = DataLoader(testset, batch_size= len(testset), shuffle=False, num_workers=4)\n",
    "        \n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        learning_rate = 1e-2\n",
    "        epochs = 6000\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        scaler_Y = GlobalScaler(trainset[:][1])\n",
    "        print('scaler_Y:' + str(scaler_Y.abs_max))\n",
    "    \n",
    "        # training\n",
    "        k = 0\n",
    "        early_stopper = EarlyStopper(nl, nch, ks, dil, patience=30, min_delta=0.0)\n",
    "        for t in range(epochs):\n",
    "            train_loss = train_loop(train_dataloader, model, nn.MSELoss(), optimizer)\n",
    "            train_losses.append(train_loss)\n",
    "            val_loss = val_loop(val_dataloader, model, nn.MSELoss())\n",
    "            val_losses.append(val_loss)\n",
    "            if t % 100 ==0:\n",
    "                print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "                print(val_loss)\n",
    "                # print(train_loss)\n",
    "            if early_stopper.early_stop(val_loss):\n",
    "                if k <8:\n",
    "                    early_stopper = EarlyStopper(nl, nch, ks, dil, patience=30, min_delta=0.0)\n",
    "                    learning_rate = learning_rate * 0.25\n",
    "                    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "                    k += 1\n",
    "                    print(\"New Learning Rate: \" + str(learning_rate))\n",
    "                else:\n",
    "                    break\n",
    "        print(\"Done!\")\n",
    "    \n",
    "        model.load_state_dict(torch.load('/glade/derecho/scratch/pahlavan/qbo1d/Nonlocality-0.5x/NNs/Model/CNN_' + \n",
    "                           str(nl) + 'L_' + str(nch) + 'C_k' + str(ks) + '_D' + str(dil) + '_0.5x_' + str(rez) + 'm.pth'))\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for batch, (X, Y) in enumerate(test_dataloader):\n",
    "                pred = scaler_Y.inverse_transform(model(X))\n",
    "    \n",
    "        f_array = xr.DataArray(pred.cpu(), dims=('time', 'height'))\n",
    "        dataset = xr.Dataset({'f': (['time', 'height'], f_array.data, [('units', 'ms-2')])})\n",
    "        dataset.to_netcdf('/glade/derecho/scratch/pahlavan/qbo1d/Nonlocality-0.5x/NNs/Data/CNN_' + \n",
    "                           str(nl) + 'L_' + str(nch) + 'C_k' + str(ks) + '_D' + str(dil) + '_0.5x_' + str(rez) + 'm_offline.nc')\n",
    "        print((((pred - (test_dataloader.dataset[:][1]).to(device))**2).mean())**0.5)\n",
    "        print((np.corrcoef(pred.cpu().flatten(), (test_dataloader.dataset[:][1]).cpu().flatten())[0][1])**2)\n",
    "        print('###############')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae6ea93-e5ee-4101-a7c4-31de2d92f6dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4a8b9b-565e-42a8-a865-f5ee5d797b62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "torch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
